#!/usr/bin/env python

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.

from __future__ import print_function

import os
import json
import pickle
import sys
import traceback

import logging

import numpy as np
import lightgbm as lgb

from hyperopt import hp, tpe
from hyperopt.fmin import fmin
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, confusion_matrix
from sklearn.model_selection import cross_val_score, StratifiedKFold
from preprocess_data import build_train

import pandas as pd

from sklearn import tree

from train.train_hyperopt_mlflow import LGBOptimizer

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a single channel of input data called 'training'. Since we run in
# File mode, the input files are copied to the directory specified here.
channel_name='training'
training_path = os.path.join(input_path, channel_name)

MODEL_FILENAME = "lightgbm-0.pkl"

def best_threshold(y_true, pred_proba, proba_range, verbose=False):
    """
    Function to find the probability threshold that optimises the f1_score

    Comment: this function is not used in this excercise, but we include it in
    case the reader finds it useful

    Parameters:
    -----------
    y_true: numpy.ndarray
        array with the true labels
    pred_proba: numpy.ndarray
        array with the predicted probability
    proba_range: numpy.ndarray
        range of probabilities to explore.
        e.g. np.arange(0.1,0.9,0.01)

    Return:
    -----------
    tuple with the optimal threshold and the corresponding f1_score
    """
    scores = []
    for prob in proba_range:
        pred = [int(p>prob) for p in pred_proba]
        score = f1_score(y_true,pred)
        scores.append(score)
        if verbose:
            print("INFO: prob threshold: {}.  score :{}".format(round(prob,3), round(score,5)))
    best_score = scores[np.argmax(scores)]
    optimal_threshold = proba_range[np.argmax(scores)]
    return (optimal_threshold, best_score)


def lgb_f1_score(preds, lgbDataset):
    """
    Function to compute the f1_score to be used with lightgbm methods.
    Comments: output format must be:
    (eval_name, eval_result, is_higher_better)

    Parameters:
    -----------
    preds: np.array or List
    lgbDataset: lightgbm.Dataset
    """
    binary_preds = [int(p>0.5) for p in preds]
    y_true = lgbDataset.get_label()
    # lightgbm: (eval_name, eval_result, is_higher_better)
    return 'f1', f1_score(y_true, binary_preds), True


class LGBOptimizer(object):
    def __init__(self, trainDataset, out_dir):
        """
        Hyper Parameter optimization

        Parameters:
        -----------
        trainDataset: FeatureTools object
            The result of running FeatureTools().fit()
        out_dir: pathlib.PosixPath
            Path to the output directory
        """
        self.PATH = out_dir
        self.early_stop_dict = {}

        self.X = trainDataset.data
        self.y = trainDataset.target
        self.colnames = trainDataset.colnames
        self.categorical_columns = trainDataset.categorical_columns + trainDataset.crossed_columns

        self.lgtrain = lgb.Dataset(self.X,label=self.y,
            feature_name=self.colnames,
            categorical_feature = self.categorical_columns,
            free_raw_data=False)

    def optimize(self, maxevals=200, model_id=0, reuse_experiment=False):

        param_space = self.hyperparameter_space()
        objective = self.get_objective(self.lgtrain)
        objective.i=0
        trials = Trials()
        best = fmin(fn=objective,
                    space=param_space,
                    algo=tpe.suggest,
                    max_evals=maxevals,
                    trials=trials)
        best['num_boost_round'] = self.early_stop_dict[trials.best_trial['tid']]
        best['num_leaves'] = int(best['num_leaves'])
        best['verbose'] = -1

        # The next few lines are the only ones related to mlflow.
        if not Path('mlruns').exists():
            # here set the tracking_uri. If None then http://localhost:5000
            client = MlflowClient()
            n_experiments=0
        elif not reuse_experiment:
            client = MlflowClient()
            n_experiments = len(client.list_experiments())
            experiment_name = 'experiment_' + str(n_experiments)
            client.create_experiment(name=experiment_name)
        with mlflow.start_run(experiment_id=n_experiments):
            model = lgb.LGBMClassifier(**best)
            model.fit(self.lgtrain.data,
                self.lgtrain.label,
                feature_name=self.colnames,
                categorical_feature=self.categorical_columns)
            for name, value in best.items():
                mlflow.log_param(name, value)
            mlflow.log_metric('binary_logloss', trials.best_trial['result']['loss'])
            mlflow.sklearn.log_model(model, "model")

        model_fname = 'model_{}_.p'.format(model_id)
        best_experiment_fname = 'best_experiment_{}_.p'.format(model_id)

        model_path = self.PATH/model_fname
        pickle.dump(model, open(model_path, 'wb'))
        pickle.dump(best, open(self.PATH/best_experiment_fname, 'wb'))

        self.best = best
        self.model = model

        return model_path

    def get_objective(self, train):

        def objective(params):
            """
            objective function for lightgbm.
            """
            #Â hyperopt casts as float
            params['num_boost_round'] = int(params['num_boost_round'])
            params['num_leaves'] = int(params['num_leaves'])

            # need to be passed as parameter
            params['is_unbalance'] = True
            params['verbose'] = -1
            params['seed'] = 1

            cv_result = lgb.cv(
                params,
                train,
                num_boost_round=params['num_boost_round'],
                metrics='binary_logloss',
                # feval = lgb_f1_score,
                nfold=3,
                stratified=True,
                early_stopping_rounds=20)
            self.early_stop_dict[objective.i] = len(cv_result['binary_logloss-mean'])
            error = cv_result['binary_logloss-mean'][-1]
            objective.i+=1
            return error

        return objective

    def hyperparameter_space(self, param_space=None):

        space = {
            'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),
            'num_boost_round': hp.quniform('num_boost_round', 50, 500, 20),
            'num_leaves': hp.quniform('num_leaves', 31, 256, 4),
            'min_child_weight': hp.uniform('min_child_weight', 0.1, 10),
            'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.),
            'subsample': hp.uniform('subsample', 0.5, 1.),
            'reg_alpha': hp.uniform('reg_alpha', 0.01, 0.1),
            'reg_lambda': hp.uniform('reg_lambda', 0.01, 0.1),
        }

        if param_space:
            return param_space
        else:
            return space


# The function to execute the training.
def train():
    print('Starting the training.')
    try:
        print("RETRAINING STARTED (model id: {})".format(model_id))

        TRAIN_DATA = '/opt/ml/input/data/training/train.csv'

        dtrain = build_train(TRAIN_DATA, DATAPROCESSORS_PATH, model_id, messages)
        LGBOpt = LGBOptimizer(dtrain, MODELS_PATH)
        model_path = LGBOpt.optimize(maxevals=2, model_id=model_id)

        # save the model
        with open(os.path.join(model_path, MODEL_FILENAME), 'wb') as out:
            pickle.dump(model, out)

        print("RETRAINING COMPLETED (model id: {})".format(model_id))
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
