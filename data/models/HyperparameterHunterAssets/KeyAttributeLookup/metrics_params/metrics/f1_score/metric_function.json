{"38f-qtDMO1jtUrmqzv5u6wTLvGX7DAOm64YSd10iQaU=": "def f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary',\n             sample_weight=None):\n    \"\"\"Compute the F1 score, also known as balanced F-score or F-measure\n\n    The F1 score can be interpreted as a weighted average of the precision and\n    recall, where an F1 score reaches its best value at 1 and worst score at 0.\n    The relative contribution of precision and recall to the F1 score are\n    equal. The formula for the F1 score is::\n\n        F1 = 2 * (precision * recall) / (precision + recall)\n\n    In the multi-class and multi-label case, this is the average of\n    the F1 score of each class with weighting depending on the ``average``\n    parameter.\n\n    Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n\n    Parameters\n    ----------\n    y_true : 1d array-like, or label indicator array / sparse matrix\n        Ground truth (correct) target values.\n\n    y_pred : 1d array-like, or label indicator array / sparse matrix\n        Estimated targets as returned by a classifier.\n\n    labels : list, optional\n        The set of labels to include when ``average != 'binary'``, and their\n        order if ``average is None``. Labels present in the data can be\n        excluded, for example to calculate a multiclass average ignoring a\n        majority negative class, while labels not present in the data will\n        result in 0 components in a macro average. For multilabel targets,\n        labels are column indices. By default, all labels in ``y_true`` and\n        ``y_pred`` are used in sorted order.\n\n        .. versionchanged:: 0.17\n           parameter *labels* improved for multiclass problem.\n\n    pos_label : str or int, 1 by default\n        The class to report if ``average='binary'`` and the data is binary.\n        If the data are multiclass or multilabel, this will be ignored;\n        setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n        scores for that label only.\n\n    average : string, [None, 'binary' (default), 'micro', 'macro', 'samples', \\\n                       'weighted']\n        This parameter is required for multiclass/multilabel targets.\n        If ``None``, the scores for each class are returned. Otherwise, this\n        determines the type of averaging performed on the data:\n\n        ``'binary'``:\n            Only report results for the class specified by ``pos_label``.\n            This is applicable only if targets (``y_{true,pred}``) are binary.\n        ``'micro'``:\n            Calculate metrics globally by counting the total true positives,\n            false negatives and false positives.\n        ``'macro'``:\n            Calculate metrics for each label, and find their unweighted\n            mean.  This does not take label imbalance into account.\n        ``'weighted'``:\n            Calculate metrics for each label, and find their average weighted\n            by support (the number of true instances for each label). This\n            alters 'macro' to account for label imbalance; it can result in an\n            F-score that is not between precision and recall.\n        ``'samples'``:\n            Calculate metrics for each instance, and find their average (only\n            meaningful for multilabel classification where this differs from\n            :func:`accuracy_score`).\n\n    sample_weight : array-like of shape = [n_samples], optional\n        Sample weights.\n\n    Returns\n    -------\n    f1_score : float or array of float, shape = [n_unique_labels]\n        F1 score of the positive class in binary classification or weighted\n        average of the F1 scores of each class for the multiclass task.\n\n    References\n    ----------\n    .. [1] `Wikipedia entry for the F1-score\n           <https://en.wikipedia.org/wiki/F1_score>`_\n\n    Examples\n    --------\n    >>> from sklearn.metrics import f1_score\n    >>> y_true = [0, 1, 2, 0, 1, 2]\n    >>> y_pred = [0, 2, 1, 0, 0, 1]\n    >>> f1_score(y_true, y_pred, average='macro')  # doctest: +ELLIPSIS\n    0.26...\n    >>> f1_score(y_true, y_pred, average='micro')  # doctest: +ELLIPSIS\n    0.33...\n    >>> f1_score(y_true, y_pred, average='weighted')  # doctest: +ELLIPSIS\n    0.26...\n    >>> f1_score(y_true, y_pred, average=None)\n    array([0.8, 0. , 0. ])\n\n\n    \"\"\"\n    return fbeta_score(y_true, y_pred, 1, labels=labels,\n                       pos_label=pos_label, average=average,\n                       sample_weight=sample_weight)\n"}